{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1919e0e0-49f9-4b45-8757-24fbe5b6939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from Stochastic_UPM_env import Factory\n",
    "from Stochastic_UPM_sim import Factory as sim_Factory\n",
    "from TransformerModel import TransformerModel \n",
    "from TransformerModel import CustomSchedule\n",
    "import pandas as pd\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7091e7ce-4129-4949-a28e-24bdd9adc48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboardX\n",
    "import time\n",
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(f'C:/Users/cimlab/UPM_stochastic/log/log-{time.time()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ec0168-aef2-4630-a02e-b2b1f22563d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, maxlen):\n",
    "        self.priority_scale = 0.8\n",
    "        self.beta = 0.4 # initial beta\n",
    "        self.beta_increment_per_sampling = 1e-4\n",
    "        self.buffer = deque(maxlen=maxlen)\n",
    "        self.priorities = deque(maxlen=maxlen) \n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(max(self.priorities, default=1)) #new experience has higher prob\n",
    "        \n",
    "    def get_probabilities(self):\n",
    "        scaled_priorities = np.array(self.priorities)**self.priority_scale\n",
    "        probs = scaled_priorities/sum(scaled_priorities)\n",
    "        return probs\n",
    "    \n",
    "    def get_importance(self, probabilities):\n",
    "        self.beta = np.min([1, self.beta + self.beta_increment_per_sampling])  # max = 1\n",
    "        importance = (1/len(self.buffer) * 1/probabilities)**self.beta\n",
    "        importance_normalized = importance / max(importance)\n",
    "        return importance_normalized\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        sample_probs = self.get_probabilities()\n",
    "        indices = np.arange(len(self.buffer))\n",
    "        sample_indices = random.choices(indices, k = batch_size, weights=sample_probs)\n",
    "        samples = np.array(self.buffer, dtype = object)[sample_indices]\n",
    "        importance = self.get_importance(sample_probs[sample_indices])\n",
    "        \n",
    "        return map(np.array, zip(*samples)), importance, indices\n",
    "    \n",
    "    def set_priorities(self, indices, errors, offset=0.1):\n",
    "        for i,e in zip(indices, errors):\n",
    "            self.priorities[i] = abs(e) + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c3f0a68-ba44-441b-a729-39cb2ff97c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_agent:\n",
    "    def __init__(self, n_states, n_actions, d_model, n_encder_lyer, n_heads, dff, s_length, dmlp):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        #self.q_network = self.build_q_network()\n",
    "        #self.t_q_network = self.build_q_network()\n",
    "        self.q_network = TransformerModel(n_actions,n_encder_lyer,d_model,n_heads,dff,s_length,dmlp)\n",
    "        self.t_q_network = TransformerModel(n_actions,n_encder_lyer,d_model,n_heads,dff,s_length,dmlp)\n",
    "        self.buffer = PrioritizedReplayBuffer(200000)\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate = CustomSchedule(d_model), clipnorm=1.0)\n",
    "        self.batch_size = 32\n",
    "        # timestep in an episode\n",
    "        self.frame_count = 0\n",
    "        # prob for exploration\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.1\n",
    "        # for epsilon decay\n",
    "        self.epsilon_greedy_frames = 120000.0\n",
    "        # discounted ratio\n",
    "        self.gamma = 0.99\n",
    "    \"\"\"\n",
    "    def build_q_network(self):\n",
    "        # Network architecture\n",
    "        inputs = keras.Input(shape = self.n_states)\n",
    "        x = layers.Conv2D(16, 3, strides = 1, activation = 'relu')(inputs)\n",
    "        x = layers.Conv2D(16, 3, strides = 1, activation = 'relu')(x)\n",
    "\n",
    "        x = layers.Conv2D(32, 3, strides = 1, activation = 'relu')(x)\n",
    "        x = layers.Conv2D(32, 3, strides = 1, activation = 'relu')(x)\n",
    "        x = layers.Flatten()(x)\n",
    "\n",
    "        x = layers.Dense(units = 256, activation = 'relu')(x)\n",
    "        q_value = layers.Dense(units = self.n_actions)(x)\n",
    "\n",
    "        return keras.Model(inputs = inputs, outputs = q_value)\n",
    "    \"\"\"\n",
    "    def choose_action(self, state, legal_one_hot):\n",
    "        # exploration and exploitation\n",
    "        if  self.epsilon >= np.random.rand(1)[0]:\n",
    "            legal = [row for row in state if row[0] != 0]\n",
    "            action = np.random.choice(len(legal))\n",
    "        else:\n",
    "            action_values = self.q_network(np.expand_dims(state, axis=(0)))\n",
    "            legal_values = legal_one_hot*action_values\n",
    "            action = np.argmax(np.where(legal_values != 0,legal_values,-np.inf))\n",
    "\n",
    "        return action\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        # decay probability of taking random action\n",
    "        self.epsilon -= (1.0 - self.epsilon_min)/self.epsilon_greedy_frames\n",
    "        self.epsilon = max(self.epsilon, self.epsilon_min)\n",
    "\n",
    "    def store(self, state, action, next_state, reward, done, next_legal):\n",
    "        # store training data\n",
    "        self.buffer.add((state, action, reward, next_state, done, next_legal))\n",
    "    \n",
    "\n",
    "    def train_q_network(self):\n",
    "        # sample\n",
    "        (states, actions, rewards, next_states, dones, next_legal), importance, \\\n",
    "            indices = self.buffer.sample(self.batch_size)\n",
    "\n",
    "        next_values = next_legal*self.q_network.predict(next_states)\n",
    "        next_action = tf.math.argmax(tf.where(next_values != 0,next_values,-np.inf), 1)\n",
    "        future_rewards = self.t_q_network.predict(next_states)\n",
    "        mask_next_action = tf.one_hot(next_action, self.n_actions)\n",
    "        # Q value = reward + discount factor * expected future reward\n",
    "        updated_q_values = rewards + self.gamma * tf.reduce_sum(tf.multiply(future_rewards, mask_next_action), axis=1)\n",
    "        \n",
    "        # set last q value to 0\n",
    "        updated_q_values = updated_q_values*(1 - dones)\n",
    "        masks = tf.one_hot(actions, self.n_actions)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "          # Train the model on the states and updated Q-values\n",
    "          q_values = self.q_network(states)\n",
    "          # only update q-value which is chosen\n",
    "          q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "          # calculate loss between new Q-value and old Q-value\n",
    "          loss = tf.reduce_mean(importance * tf.math.square(q_action - updated_q_values))\n",
    "        \n",
    "        # set priorities\n",
    "        errors = updated_q_values - q_action\n",
    "        self.buffer.set_priorities(indices, errors)\n",
    "        \n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))\n",
    "\n",
    "    def update_target_network(self):\n",
    "        # update per update_target_network steps\n",
    "        self.t_q_network.set_weights(self.q_network.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d0b0876-0c6a-4c70-bfa8-3df09095cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 9\n",
    "n_actions = 20\n",
    "n_encder_lyer = 2\n",
    "d_model = 16\n",
    "n_heads = 2\n",
    "dff = 32\n",
    "dmlp = 32\n",
    "warm_up_time = 10080\n",
    "update_per_actions = 4\n",
    "max_steps_per_episode = 1000\n",
    "update_target_network = 1000\n",
    "agent = DQN_agent(n_states, n_actions, d_model, n_encder_lyer, n_heads, dff, n_actions, dmlp)\n",
    "env = Factory(warm_up_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baed7924-4b84-4f0e-8acb-15abd2800748",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 0\n",
    "total_episode = 800\n",
    "while True:\n",
    "    episode += 1\n",
    "    state, legal = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        agent.frame_count += 1\n",
    "        # choose action\n",
    "        action = agent.choose_action(state, legal)\n",
    "        # decay prob of exploration\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "        next_state, reward, done, next_legal, inf = env.step(action)\n",
    "\n",
    "        episode_reward += reward\n",
    "        # store training data\n",
    "        agent.store(\n",
    "            state, action, next_state, reward, done, next_legal\n",
    "            )\n",
    "\n",
    "        state = next_state\n",
    "        legal = next_legal\n",
    "        \n",
    "        if agent.frame_count % update_per_actions == 0 and len(agent.buffer.buffer) >= agent.batch_size:\n",
    "            agent.train_q_network()\n",
    "\n",
    "        if agent.frame_count % update_target_network == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "        if done:\n",
    "            tardy_job_percentage = inf\n",
    "            break\n",
    "        \n",
    "    writer.add_scalar('Reward',episode_reward, episode)\n",
    "    writer.add_scalar('Tardy jobs', tardy_job_percentage,  episode)\n",
    "    writer.add_scalar('Epsilon', agent.epsilon,  episode)\n",
    "\n",
    "    if episode >= total_episode:\n",
    "        break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7949931f-cf54-4eae-b26d-4994de355761",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.q_network.save_weights('transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f16f7c6-d170-49ff-a98f-ab2c9f2ee28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4155, 1126, 8704, 5692, 901, 9806, 261, 7851, 9011, 9638, 8684, 9074, 2945, 4670, 8522, 68, 1381, 7087, 6076, 1854, 1955, 5046, 2565, 8639, 9165, 2359, 2644, 7725, 5971, 435, 2549, 6884, 9634, 3575, 3775, 5175, 4986, 3054, 1484, 761, 8601, 8521, 7777, 8593, 3020, 2517, 4991, 9315, 8760, 5517, 7140, 8934, 1206, 1857, 4623, 2912, 779, 6524, 5637, 4515, 9949, 2702, 8901, 4041, 9380, 5320, 4046, 3247, 5847, 5381, 5898, 907, 2770, 4535, 7502, 2481, 245, 136, 9101, 5570, 3291, 4259, 721, 8461, 2840, 9882, 5064, 9102, 1565, 4801, 421, 3966, 960, 5149, 3840, 1905, 3234, 1923, 2585, 7370]\n"
     ]
    }
   ],
   "source": [
    "replication = 100\n",
    "random_seeds = random.sample(range(1,10000),replication)\n",
    "print(random_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d359f1b-209b-4767-8962-86dda8b53dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = np.zeros((replication,9))\n",
    "agent.epsilon = 0\n",
    "for i in range(replication):\n",
    "    performance[i,0] = random_seeds[i]\n",
    "    np.random.seed(random_seeds[i])\n",
    "    state, legal = env.reset()\n",
    "    while True:\n",
    "        action = agent.choose_action(state, legal)\n",
    "\n",
    "        next_state, reward, done, next_legal, inf = env.step(action)\n",
    "\n",
    "        state = next_state\n",
    "        legal = next_legal\n",
    "\n",
    "        if done:\n",
    "            performance[i,-1] = inf\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8715e102-f602-415d-babc-04d3d302d76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPM = sim_Factory()\n",
    "for i in range(replication):\n",
    "    np.random.seed(random_seeds[i])\n",
    "    for j in range(7):\n",
    "        UPM.build(j)\n",
    "        UPM.warm_up(warm_up_time)\n",
    "        UPM.env.run(until = UPM.terminal)\n",
    "        performance[i,j+1] = np.sum(UPM.sink.number_of_late)/UPM.sink.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82d4cc76-0bad-459f-870b-208ea0dae653",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(performance,columns=['Random seed','SPT','EDD','MST','ST','CR','WSPT','FIFO','DQN']).to_csv('experiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a798b87-1d5f-4e53-8433-597576afabf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_confidence_interval(a, confidence=0.95):\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    std = np.std(a, ddof = 1)\n",
    "    return  m-h, m, m+h, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd04f45e-76ea-4ecf-a108-b8041995c0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>95%CI LOWER</th>\n",
       "      <th>MEAN</th>\n",
       "      <th>95%CI UPPER</th>\n",
       "      <th>STD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SPT</th>\n",
       "      <td>0.288717</td>\n",
       "      <td>0.32040</td>\n",
       "      <td>0.352083</td>\n",
       "      <td>0.159677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDD</th>\n",
       "      <td>0.302248</td>\n",
       "      <td>0.33890</td>\n",
       "      <td>0.375552</td>\n",
       "      <td>0.184720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MST</th>\n",
       "      <td>0.283407</td>\n",
       "      <td>0.31395</td>\n",
       "      <td>0.344493</td>\n",
       "      <td>0.153930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST</th>\n",
       "      <td>0.268978</td>\n",
       "      <td>0.29740</td>\n",
       "      <td>0.325822</td>\n",
       "      <td>0.143243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CR</th>\n",
       "      <td>0.304809</td>\n",
       "      <td>0.33655</td>\n",
       "      <td>0.368291</td>\n",
       "      <td>0.159968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WSPT</th>\n",
       "      <td>0.286247</td>\n",
       "      <td>0.31480</td>\n",
       "      <td>0.343353</td>\n",
       "      <td>0.143903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIFO</th>\n",
       "      <td>0.283810</td>\n",
       "      <td>0.31880</td>\n",
       "      <td>0.353790</td>\n",
       "      <td>0.176341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DQN</th>\n",
       "      <td>0.227857</td>\n",
       "      <td>0.24075</td>\n",
       "      <td>0.253643</td>\n",
       "      <td>0.064980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      95%CI LOWER     MEAN  95%CI UPPER       STD\n",
       "SPT      0.288717  0.32040     0.352083  0.159677\n",
       "EDD      0.302248  0.33890     0.375552  0.184720\n",
       "MST      0.283407  0.31395     0.344493  0.153930\n",
       "ST       0.268978  0.29740     0.325822  0.143243\n",
       "CR       0.304809  0.33655     0.368291  0.159968\n",
       "WSPT     0.286247  0.31480     0.343353  0.143903\n",
       "FIFO     0.283810  0.31880     0.353790  0.176341\n",
       "DQN      0.227857  0.24075     0.253643  0.064980"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_str = ['SPT','EDD','MST','ST','CR','WSPT','FIFO','DQN']\n",
    "performance_trans = performance.transpose()\n",
    "statistics = pd.DataFrame(columns = ['95%CI LOWER','MEAN','95%CI UPPER','STD'])\n",
    "for i in range(len(performance_trans)-1):\n",
    "    l, m, u, se = mean_confidence_interval(performance_trans[i+1], confidence=0.95)\n",
    "    statistics_row=pd.DataFrame([[l, m, u, se]],columns=['95%CI LOWER','MEAN','95%CI UPPER','STD']\n",
    "                                ,index = [performance_str[i]])\n",
    "    statistics=statistics.append(statistics_row)\n",
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a11418-752d-4453-9e2f-04c83c1322ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
