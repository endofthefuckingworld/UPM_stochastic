{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08e4262c-f30c-4520-b4ca-5181d53f8f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from Stochastic_UPM_env import Factory\n",
    "from Stochastic_UPM_sim import Factory as sim_Factory\n",
    "import pandas as pd\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c577ba99-c0a9-4c5d-9240-386efc78f126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6306, 7820, 5087, 9954, 2286, 368, 5894, 8362, 9208, 7394, 9710, 1450, 4655, 3079, 1908, 1103, 6288, 1412, 8780, 3450, 8624, 9866, 1078, 1859, 7093, 2437, 980, 7521, 6877, 7265, 2221, 2017, 8713, 1286, 3085, 5110, 5614, 8152, 4551, 5868, 5666, 7543, 4927, 4431, 4960, 4752, 9957, 1271, 9878, 5766, 5585, 6908, 2951, 9508, 1793, 5069, 2675, 4644, 5396, 2552, 7344, 5294, 1425, 1305, 7200, 5106, 7050, 464, 9978, 8803, 3023, 5794, 6785, 4281, 2562, 788, 4756, 9651, 864, 4594, 6245, 424, 9939, 3895, 2416, 5143, 5100, 5598, 4979, 8786, 4874, 971, 3386, 9179, 1392, 268, 2180, 6636, 6715, 1545]\n"
     ]
    }
   ],
   "source": [
    "replication = 100\n",
    "random_seeds = random.sample(range(1,10000),replication)\n",
    "print(random_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a050a9fb-6878-413e-a79b-c65044aa8fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, maxlen):\n",
    "        self.priority_scale = 0.8\n",
    "        self.beta = 0.4 # initial beta\n",
    "        self.beta_increment_per_sampling = 1e-4\n",
    "        self.buffer = deque(maxlen=maxlen)\n",
    "        self.priorities = deque(maxlen=maxlen) \n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(max(self.priorities, default=1)) #new experience has higher prob\n",
    "        \n",
    "    def get_probabilities(self):\n",
    "        scaled_priorities = np.array(self.priorities)**self.priority_scale\n",
    "        probs = scaled_priorities/sum(scaled_priorities)\n",
    "        return probs\n",
    "    \n",
    "    def get_importance(self, probabilities):\n",
    "        self.beta = np.min([1, self.beta + self.beta_increment_per_sampling])  # max = 1\n",
    "        importance = (1/len(self.buffer) * 1/probabilities)**self.beta\n",
    "        importance_normalized = importance / max(importance)\n",
    "        return importance_normalized\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        sample_probs = self.get_probabilities()\n",
    "        indices = np.arange(len(self.buffer))\n",
    "        sample_indices = random.choices(indices, k = batch_size, weights=sample_probs)\n",
    "        samples = np.array(self.buffer, dtype = object)[sample_indices]\n",
    "        importance = self.get_importance(sample_probs[sample_indices])\n",
    "        \n",
    "        return map(np.array, zip(*samples)), importance, indices\n",
    "    \n",
    "    def set_priorities(self, indices, errors, offset=0.1):\n",
    "        for i,e in zip(indices, errors):\n",
    "            self.priorities[i] = abs(e) + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5621c99-de1d-4b57-8a06-8a6f48a1f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_agent:\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.q_network = self.build_q_network()\n",
    "        self.t_q_network = self.build_q_network()\n",
    "        self.buffer = PrioritizedReplayBuffer(200000)\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate = 3e-4, clipnorm=1.0)\n",
    "        self.batch_size = 32\n",
    "        # timestep in an episode\n",
    "        self.frame_count = 0\n",
    "        # prob for exploration\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.1\n",
    "        # for epsilon decay\n",
    "        self.epsilon_greedy_frames = 80000.0\n",
    "        # discounted ratio\n",
    "        self.gamma = 0.99\n",
    "    \n",
    "    def build_q_network(self):\n",
    "        # Network architecture\n",
    "        inputs = keras.Input(shape = self.n_states)\n",
    "        x = layers.Conv2D(16, 3, strides = 1, activation = 'relu')(inputs)\n",
    "        x = layers.Conv2D(16, 3, strides = 1, activation = 'relu')(x)\n",
    "\n",
    "        x = layers.Conv2D(32, 3, strides = 1, activation = 'relu')(x)\n",
    "        x = layers.Conv2D(32, 3, strides = 1, activation = 'relu')(x)\n",
    "        x = layers.Flatten()(x)\n",
    "\n",
    "        x = layers.Dense(units = 256, activation = 'relu')(x)\n",
    "        q_value = layers.Dense(units = self.n_actions)(x)\n",
    "\n",
    "        return keras.Model(inputs = inputs, outputs = q_value)\n",
    "    \n",
    "    def choose_action(self, state, legal_one_hot):\n",
    "        # exploration and exploitation\n",
    "        if  self.epsilon >= np.random.rand(1)[0]:\n",
    "            legal = [row for row in state if row[0] != 0]\n",
    "            action = np.random.choice(len(legal))\n",
    "        else:\n",
    "            action_values = self.q_network(np.expand_dims(state, axis=(0,-1)))\n",
    "            legal_values = legal_one_hot*action_values\n",
    "            action = np.argmax(np.where(legal_values != 0,legal_values,-np.inf))\n",
    "\n",
    "        return action\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        # decay probability of taking random action\n",
    "        self.epsilon -= (1.0 - self.epsilon_min)/self.epsilon_greedy_frames\n",
    "        self.epsilon = max(self.epsilon, self.epsilon_min)\n",
    "\n",
    "    def store(self, state, action, next_state, reward, done, next_legal):\n",
    "        # store training data\n",
    "        self.buffer.add((state, action, reward, next_state, done, next_legal))\n",
    "    \n",
    "\n",
    "    def train_q_network(self):\n",
    "        # sample\n",
    "        (states, actions, rewards, next_states, dones, next_legal), importance, \\\n",
    "            indices = self.buffer.sample(self.batch_size)\n",
    "\n",
    "        next_values = next_legal*self.q_network.predict(next_states)\n",
    "        next_action = tf.math.argmax(tf.where(next_values != 0,next_values,-np.inf), 1)\n",
    "        future_rewards = self.t_q_network.predict(next_states)\n",
    "        mask_next_action = tf.one_hot(next_action, self.n_actions)\n",
    "        # Q value = reward + discount factor * expected future reward\n",
    "        updated_q_values = rewards + self.gamma * tf.reduce_sum(tf.multiply(future_rewards, mask_next_action), axis=1)\n",
    "        \n",
    "        # set last q value to 0\n",
    "        updated_q_values = updated_q_values*(1 - dones)\n",
    "        masks = tf.one_hot(actions, self.n_actions)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "          # Train the model on the states and updated Q-values\n",
    "          q_values = self.q_network(states)\n",
    "          # only update q-value which is chosen\n",
    "          q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "          # calculate loss between new Q-value and old Q-value\n",
    "          loss = tf.reduce_mean(importance * tf.math.square(q_action - updated_q_values))\n",
    "        \n",
    "        # set priorities\n",
    "        errors = updated_q_values - q_action\n",
    "        self.buffer.set_priorities(indices, errors)\n",
    "        \n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))\n",
    "\n",
    "    def update_target_network(self):\n",
    "        # update per update_target_network steps\n",
    "        self.t_q_network.set_weights(self.q_network.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab96e0cd-8ec2-4db7-9072-1f9043023f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "n_states = (40,9,1)\n",
    "n_actions = 40\n",
    "warm_up_time = 10080\n",
    "update_per_actions = 4\n",
    "max_steps_per_episode = 1000\n",
    "update_target_network = 1000\n",
    "agent = DQN_agent(n_states, n_actions)\n",
    "env = Factory(warm_up_time)\n",
    "agent.q_network = keras.models.load_model('Stocahstic_model.h5')\n",
    "performance = np.zeros((replication,9))\n",
    "agent.epsilon = 0\n",
    "for i in range(replication):\n",
    "    performance[i,0] = random_seeds[i]\n",
    "    np.random.seed(random_seeds[i])\n",
    "    state, legal = env.reset()\n",
    "    while True:\n",
    "        action = agent.choose_action(state, legal)\n",
    "\n",
    "        next_state, reward, done, next_legal, inf = env.step(action)\n",
    "\n",
    "        state = next_state\n",
    "        legal = next_legal\n",
    "\n",
    "        if done:\n",
    "            performance[i,-1] = inf\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c49eda2-c324-40d3-a83e-4676a0becb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPM = sim_Factory()\n",
    "for i in range(replication):\n",
    "    np.random.seed(random_seeds[i])\n",
    "    for j in range(7):\n",
    "        UPM.build(j)\n",
    "        UPM.warm_up(warm_up_time)\n",
    "        UPM.env.run(until = UPM.terminal)\n",
    "        performance[i,j+1] = np.sum(UPM.sink.number_of_late)/UPM.sink.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba05d8fc-c68f-4bb4-acd4-d4cce702990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(performance,columns=['Random seed','SPT','EDD','MST','ST','CR','WSPT','FIFO','DQN']).to_csv('experiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40a98083-d388-479f-b98b-8aa3069daf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_confidence_interval(a, confidence=0.95):\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    std = np.std(a, ddof = 1)\n",
    "    return  m-h, m, m+h, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfa256c8-be90-4378-bcad-765249bb971e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>95%CI LOWER</th>\n",
       "      <th>MEAN</th>\n",
       "      <th>95%CI UPPER</th>\n",
       "      <th>STD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SPT</th>\n",
       "      <td>0.221638</td>\n",
       "      <td>0.24360</td>\n",
       "      <td>0.265562</td>\n",
       "      <td>0.110682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDD</th>\n",
       "      <td>0.236037</td>\n",
       "      <td>0.26200</td>\n",
       "      <td>0.287963</td>\n",
       "      <td>0.130846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MST</th>\n",
       "      <td>0.232137</td>\n",
       "      <td>0.25235</td>\n",
       "      <td>0.272563</td>\n",
       "      <td>0.101868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST</th>\n",
       "      <td>0.238040</td>\n",
       "      <td>0.26150</td>\n",
       "      <td>0.284960</td>\n",
       "      <td>0.118231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CR</th>\n",
       "      <td>0.227207</td>\n",
       "      <td>0.24610</td>\n",
       "      <td>0.264993</td>\n",
       "      <td>0.095215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WSPT</th>\n",
       "      <td>0.247291</td>\n",
       "      <td>0.27170</td>\n",
       "      <td>0.296109</td>\n",
       "      <td>0.123014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIFO</th>\n",
       "      <td>0.218082</td>\n",
       "      <td>0.23910</td>\n",
       "      <td>0.260118</td>\n",
       "      <td>0.105926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DQN</th>\n",
       "      <td>0.208439</td>\n",
       "      <td>0.21795</td>\n",
       "      <td>0.227461</td>\n",
       "      <td>0.047932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      95%CI LOWER     MEAN  95%CI UPPER       STD\n",
       "SPT      0.221638  0.24360     0.265562  0.110682\n",
       "EDD      0.236037  0.26200     0.287963  0.130846\n",
       "MST      0.232137  0.25235     0.272563  0.101868\n",
       "ST       0.238040  0.26150     0.284960  0.118231\n",
       "CR       0.227207  0.24610     0.264993  0.095215\n",
       "WSPT     0.247291  0.27170     0.296109  0.123014\n",
       "FIFO     0.218082  0.23910     0.260118  0.105926\n",
       "DQN      0.208439  0.21795     0.227461  0.047932"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_str = ['SPT','EDD','MST','ST','CR','WSPT','FIFO','DQN']\n",
    "performance_trans = performance.transpose()\n",
    "statistics = pd.DataFrame(columns = ['95%CI LOWER','MEAN','95%CI UPPER','STD'])\n",
    "for i in range(len(performance_trans)-1):\n",
    "    l, m, u, se = mean_confidence_interval(performance_trans[i+1], confidence=0.95)\n",
    "    statistics_row=pd.DataFrame([[l, m, u, se]],columns=['95%CI LOWER','MEAN','95%CI UPPER','STD']\n",
    "                                ,index = [performance_str[i]])\n",
    "    statistics=statistics.append(statistics_row)\n",
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e661cd7-ade5-47e7-acbb-f8e8e9ba1611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
