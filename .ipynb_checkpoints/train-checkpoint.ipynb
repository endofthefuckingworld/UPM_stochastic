{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1919e0e0-49f9-4b45-8757-24fbe5b6939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from Stochastic_UPM_env import Factory\n",
    "from Stochastic_UPM_sim import Factory as sim_Factory\n",
    "from TransformerModel import TransformerModel \n",
    "from TransformerModel import CustomSchedule\n",
    "import pandas as pd\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7091e7ce-4129-4949-a28e-24bdd9adc48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboardX\n",
    "import time\n",
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(f'C:/Users/cimlab/UPM_stochastic/log/log-{time.time()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ec0168-aef2-4630-a02e-b2b1f22563d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, maxlen):\n",
    "        self.priority_scale = 0.8\n",
    "        self.beta = 0.4 # initial beta\n",
    "        self.beta_increment_per_sampling = 1e-4\n",
    "        self.buffer = deque(maxlen=maxlen)\n",
    "        self.priorities = deque(maxlen=maxlen) \n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(max(self.priorities, default=1)) #new experience has higher prob\n",
    "        \n",
    "    def get_probabilities(self):\n",
    "        scaled_priorities = np.array(self.priorities)**self.priority_scale\n",
    "        probs = scaled_priorities/sum(scaled_priorities)\n",
    "        return probs\n",
    "    \n",
    "    def get_importance(self, probabilities):\n",
    "        self.beta = np.min([1, self.beta + self.beta_increment_per_sampling])  # max = 1\n",
    "        importance = (1/len(self.buffer) * 1/probabilities)**self.beta\n",
    "        importance_normalized = importance / max(importance)\n",
    "        return importance_normalized\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        sample_probs = self.get_probabilities()\n",
    "        indices = np.arange(len(self.buffer))\n",
    "        sample_indices = random.choices(indices, k = batch_size, weights=sample_probs)\n",
    "        samples = np.array(self.buffer, dtype = object)[sample_indices]\n",
    "        importance = self.get_importance(sample_probs[sample_indices])\n",
    "        \n",
    "        return map(np.array, zip(*samples)), importance, indices\n",
    "    \n",
    "    def set_priorities(self, indices, errors, offset=0.1):\n",
    "        for i,e in zip(indices, errors):\n",
    "            self.priorities[i] = abs(e) + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c3f0a68-ba44-441b-a729-39cb2ff97c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_agent:\n",
    "    def __init__(self, n_states, n_actions, d_model, n_encder_lyer, n_heads, dff, s_length, dmlp):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        #self.q_network = self.build_q_network()\n",
    "        #self.t_q_network = self.build_q_network()\n",
    "        self.q_network = TransformerModel(n_actions,n_encder_lyer,d_model,n_heads,dff,s_length,dmlp)\n",
    "        self.t_q_network = TransformerModel(n_actions,n_encder_lyer,d_model,n_heads,dff,s_length,dmlp)\n",
    "        self.buffer = PrioritizedReplayBuffer(200000)\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate = CustomSchedule(d_model), clipnorm=1.0)\n",
    "        self.batch_size = 32\n",
    "        # timestep in an episode\n",
    "        self.frame_count = 0\n",
    "        # prob for exploration\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.1\n",
    "        # for epsilon decay\n",
    "        self.epsilon_greedy_frames = 120000.0\n",
    "        # discounted ratio\n",
    "        self.gamma = 0.99\n",
    "    \"\"\"\n",
    "    def build_q_network(self):\n",
    "        # Network architecture\n",
    "        inputs = keras.Input(shape = self.n_states)\n",
    "        x = layers.Conv2D(16, 3, strides = 1, activation = 'relu')(inputs)\n",
    "        x = layers.Conv2D(16, 3, strides = 1, activation = 'relu')(x)\n",
    "\n",
    "        x = layers.Conv2D(32, 3, strides = 1, activation = 'relu')(x)\n",
    "        x = layers.Conv2D(32, 3, strides = 1, activation = 'relu')(x)\n",
    "        x = layers.Flatten()(x)\n",
    "\n",
    "        x = layers.Dense(units = 256, activation = 'relu')(x)\n",
    "        q_value = layers.Dense(units = self.n_actions)(x)\n",
    "\n",
    "        return keras.Model(inputs = inputs, outputs = q_value)\n",
    "    \"\"\"\n",
    "    def choose_action(self, state, legal_one_hot):\n",
    "        # exploration and exploitation\n",
    "        if  self.epsilon >= np.random.rand(1)[0]:\n",
    "            legal = [row for row in state if row[0] != 0]\n",
    "            action = np.random.choice(len(legal))\n",
    "        else:\n",
    "            action_values = self.q_network(np.expand_dims(state, axis=(0)))\n",
    "            legal_values = legal_one_hot*action_values\n",
    "            action = np.argmax(np.where(legal_values != 0,legal_values,-np.inf))\n",
    "\n",
    "        return action\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        # decay probability of taking random action\n",
    "        self.epsilon -= (1.0 - self.epsilon_min)/self.epsilon_greedy_frames\n",
    "        self.epsilon = max(self.epsilon, self.epsilon_min)\n",
    "\n",
    "    def store(self, state, action, next_state, reward, done, next_legal):\n",
    "        # store training data\n",
    "        self.buffer.add((state, action, reward, next_state, done, next_legal))\n",
    "    \n",
    "\n",
    "    def train_q_network(self):\n",
    "        # sample\n",
    "        (states, actions, rewards, next_states, dones, next_legal), importance, \\\n",
    "            indices = self.buffer.sample(self.batch_size)\n",
    "\n",
    "        next_values = next_legal*self.q_network.predict(next_states)\n",
    "        next_action = tf.math.argmax(tf.where(next_values != 0,next_values,-np.inf), 1)\n",
    "        future_rewards = self.t_q_network.predict(next_states)\n",
    "        mask_next_action = tf.one_hot(next_action, self.n_actions)\n",
    "        # Q value = reward + discount factor * expected future reward\n",
    "        updated_q_values = rewards + self.gamma * tf.reduce_sum(tf.multiply(future_rewards, mask_next_action), axis=1)\n",
    "        \n",
    "        # set last q value to 0\n",
    "        updated_q_values = updated_q_values*(1 - dones)\n",
    "        masks = tf.one_hot(actions, self.n_actions)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "          # Train the model on the states and updated Q-values\n",
    "          q_values = self.q_network(states)\n",
    "          # only update q-value which is chosen\n",
    "          q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "          # calculate loss between new Q-value and old Q-value\n",
    "          loss = tf.reduce_mean(importance * tf.math.square(q_action - updated_q_values))\n",
    "        \n",
    "        # set priorities\n",
    "        errors = updated_q_values - q_action\n",
    "        self.buffer.set_priorities(indices, errors)\n",
    "        \n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))\n",
    "\n",
    "    def update_target_network(self):\n",
    "        # update per update_target_network steps\n",
    "        self.t_q_network.set_weights(self.q_network.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d0b0876-0c6a-4c70-bfa8-3df09095cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 9\n",
    "n_actions = 20\n",
    "n_encder_lyer = 2\n",
    "d_model = 16\n",
    "n_heads = 2\n",
    "dff = 32\n",
    "dmlp = 32\n",
    "warm_up_time = 10080\n",
    "update_per_actions = 4\n",
    "max_steps_per_episode = 1000\n",
    "update_target_network = 1000\n",
    "agent = DQN_agent(n_states, n_actions, d_model, n_encder_lyer, n_heads, dff, n_actions, dmlp)\n",
    "env = Factory(warm_up_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baed7924-4b84-4f0e-8acb-15abd2800748",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\cimlab\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1569 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\cimlab\\UPM_stochastic\\TransformerModel.py:94 call  *\n        x = self.encoder(x)\n    C:\\Users\\cimlab\\UPM_stochastic\\TransformerModel.py:75 call  *\n        x = self.prenet(self.d_model)\n    C:\\Users\\cimlab\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__  **\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\cimlab\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:230 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer dense is incompatible with the layer: : expected min_ndim=2, found ndim=0. Full shape received: ()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11056/436790.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe_count\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mupdate_per_actions\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_q_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe_count\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mupdate_target_network\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11056/3346391920.py\u001b[0m in \u001b[0;36mtrain_q_network\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mnext_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_legal\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mnext_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_values\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnext_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mfuture_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_q_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1725\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1726\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1727\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1728\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1729\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 763\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    764\u001b[0m             *args, **kwds))\n\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3049\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3050\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3051\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3444\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3279\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\cimlab\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1569 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\cimlab\\UPM_stochastic\\TransformerModel.py:94 call  *\n        x = self.encoder(x)\n    C:\\Users\\cimlab\\UPM_stochastic\\TransformerModel.py:75 call  *\n        x = self.prenet(self.d_model)\n    C:\\Users\\cimlab\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__  **\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\cimlab\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:230 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer dense is incompatible with the layer: : expected min_ndim=2, found ndim=0. Full shape received: ()\n"
     ]
    }
   ],
   "source": [
    "episode = 0\n",
    "total_episode = 800\n",
    "while True:\n",
    "    episode += 1\n",
    "    state, legal = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        agent.frame_count += 1\n",
    "        # choose action\n",
    "        action = agent.choose_action(state, legal)\n",
    "        # decay prob of exploration\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "        next_state, reward, done, next_legal, inf = env.step(action)\n",
    "\n",
    "        episode_reward += reward\n",
    "        # store training data\n",
    "        agent.store(\n",
    "            state, action, next_state, reward, done, next_legal\n",
    "            )\n",
    "\n",
    "        state = next_state\n",
    "        legal = next_legal\n",
    "        \n",
    "        if agent.frame_count % update_per_actions == 0 and len(agent.buffer.buffer) >= agent.batch_size:\n",
    "            agent.train_q_network()\n",
    "\n",
    "        if agent.frame_count % update_target_network == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "        if done:\n",
    "            tardy_job_percentage = inf\n",
    "            break\n",
    "        \n",
    "    writer.add_scalar('Reward',episode_reward, episode)\n",
    "    writer.add_scalar('Tardy jobs', tardy_job_percentage,  episode)\n",
    "    writer.add_scalar('Epsilon', agent.epsilon,  episode)\n",
    "\n",
    "    if episode >= total_episode:\n",
    "        break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7949931f-cf54-4eae-b26d-4994de355761",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.q_network.save('Stocahstic_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f16f7c6-d170-49ff-a98f-ab2c9f2ee28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "replication = 100\n",
    "random_seeds = random.sample(range(1,10000),replication)\n",
    "print(random_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d359f1b-209b-4767-8962-86dda8b53dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = np.zeros((replication,9))\n",
    "agent.epsilon = 0\n",
    "for i in range(replication):\n",
    "    performance[i,0] = random_seeds[i]\n",
    "    np.random.seed(random_seeds[i])\n",
    "    state, legal = env.reset()\n",
    "    while True:\n",
    "        action = agent.choose_action(state, legal)\n",
    "\n",
    "        next_state, reward, done, next_legal, inf = env.step(action)\n",
    "\n",
    "        state = next_state\n",
    "        legal = next_legal\n",
    "\n",
    "        if done:\n",
    "            performance[i,-1] = inf\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8715e102-f602-415d-babc-04d3d302d76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "UPM = sim_Factory()\n",
    "for i in range(replication):\n",
    "    np.random.seed(random_seeds[i])\n",
    "    for j in range(7):\n",
    "        UPM.build(j)\n",
    "        UPM.warm_up(warm_up_time)\n",
    "        UPM.env.run(until = UPM.terminal)\n",
    "        performance[i,j+1] = np.sum(UPM.sink.number_of_late)/UPM.sink.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d4cc76-0bad-459f-870b-208ea0dae653",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(performance,columns=['Random seed','SPT','EDD','MST','ST','CR','WSPT','FIFO','DQN']).to_csv('experiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a798b87-1d5f-4e53-8433-597576afabf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_confidence_interval(a, confidence=0.95):\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    std = np.std(a, ddof = 1)\n",
    "    return  m-h, m, m+h, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd04f45e-76ea-4ecf-a108-b8041995c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_str = ['SPT','EDD','MST','ST','CR','WSPT','FIFO','DQN']\n",
    "performance_trans = performance.transpose()\n",
    "statistics = pd.DataFrame(columns = ['95%CI LOWER','MEAN','95%CI UPPER','STD'])\n",
    "for i in range(len(performance_trans)-1):\n",
    "    l, m, u, se = mean_confidence_interval(performance_trans[i+1], confidence=0.95)\n",
    "    statistics_row=pd.DataFrame([[l, m, u, se]],columns=['95%CI LOWER','MEAN','95%CI UPPER','STD']\n",
    "                                ,index = [performance_str[i]])\n",
    "    statistics=statistics.append(statistics_row)\n",
    "statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a11418-752d-4453-9e2f-04c83c1322ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
