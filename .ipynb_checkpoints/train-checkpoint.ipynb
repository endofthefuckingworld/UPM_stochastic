{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1919e0e0-49f9-4b45-8757-24fbe5b6939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from Stochastic_UPM_env import Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec0168-aef2-4630-a02e-b2b1f22563d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, maxlen):\n",
    "        self.priority_scale = 0.8\n",
    "        self.beta = 0.4 # initial beta\n",
    "        self.beta_increment_per_sampling = 1e-4\n",
    "        self.buffer = deque(maxlen=maxlen)\n",
    "        self.priorities = deque(maxlen=maxlen) \n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(max(self.priorities, default=1)) #new experience has higher prob\n",
    "        \n",
    "    def get_probabilities(self):\n",
    "        scaled_priorities = np.array(self.priorities)**self.priority_scale\n",
    "        probs = scaled_priorities/sum(scaled_priorities)\n",
    "        return probs\n",
    "    \n",
    "    def get_importance(self, probabilities):\n",
    "        self.beta = np.min([1, self.beta + self.beta_increment_per_sampling])  # max = 1\n",
    "        importance = (1/len(self.buffer) * 1/probabilities)**self.beta\n",
    "        importance_normalized = importance / max(importance)\n",
    "        return importance_normalized\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        sample_probs = self.get_probabilities()\n",
    "        indices = np.arange(len(self.buffer))\n",
    "        sample_indices = random.choices(indices, k = batch_size, weights=sample_probs)\n",
    "        samples = np.array(self.buffer, dtype = object)[sample_indices]\n",
    "        importance = self.get_importance(sample_probs[sample_indices])\n",
    "        \n",
    "        return map(np.array, zip(*samples)), importance, indices\n",
    "    \n",
    "    def set_priorities(self, indices, errors, offset=0.1):\n",
    "        for i,e in zip(indices, errors):\n",
    "            self.priorities[i] = abs(e) + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3f0a68-ba44-441b-a729-39cb2ff97c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_agent:\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.q_network = self.build_q_network()\n",
    "        self.t_q_network = self.build_q_network()\n",
    "        self.buffer = PrioritizedReplayBuffer(100000)\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate = 3e-4, clipnorm=1.0)\n",
    "        self.batch_size = 32\n",
    "        # timestep in an episode\n",
    "        self.frame_count = 0\n",
    "        # prob for exploration\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.1\n",
    "        # for epsilon decay\n",
    "        self.epsilon_greedy_frames = 100000.0\n",
    "        # discounted ratio\n",
    "        self.gamma = 0.99\n",
    "    \n",
    "    def build_q_network(self):\n",
    "        # Network architecture\n",
    "        inputs = keras.Input(shape = self.n_states)\n",
    "        x = layers.Conv2D(16, 3, strides = 1, activation = 'relu')(inputs)\n",
    "        x = layers.Conv2D(16, 3, strides = 1, activation = 'relu')(x)\n",
    "\n",
    "        x = layers.Conv2D(32, 3, strides = 1, activation = 'relu')(x)\n",
    "        x = layers.Conv2D(32, 3, strides = 1, activation = 'relu')(x)\n",
    "        x = layers.Flatten()(x)\n",
    "\n",
    "        x = layers.Dense(units = 256, activation = 'relu')(x)\n",
    "        q_value = layers.Dense(units = self.n_actions)(x)\n",
    "\n",
    "        return keras.Model(inputs = inputs, outputs = q_value)\n",
    "    \n",
    "    def choose_action(self, state, legal_one_hot):\n",
    "        # exploration and exploitation\n",
    "        if  self.epsilon >= np.random.rand(1)[0]:\n",
    "            legal = [row for row in state if row[0] != 0]\n",
    "            action = np.random.choice(len(legal))\n",
    "        else:\n",
    "            action_values = self.q_network(np.expand_dims(state, axis=(0,-1)))\n",
    "            legal_values = legal_one_hot*action_values\n",
    "            action = np.argmax(np.where(legal_values != 0,legal_values,-np.inf))\n",
    "\n",
    "        return action\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        # decay probability of taking random action\n",
    "        self.epsilon -= (1.0 - self.epsilon_min)/self.epsilon_greedy_frames\n",
    "        self.epsilon = max(self.epsilon, self.epsilon_min)\n",
    "\n",
    "    def store(self, state, action, next_state, reward, done, next_legal):\n",
    "        # store training data\n",
    "        self.buffer.add((state, action, reward, next_state, done, next_legal))\n",
    "    \n",
    "\n",
    "    def train_q_network(self):\n",
    "        # sample\n",
    "        (states, actions, rewards, next_states, dones, next_legal), importance, \\\n",
    "            indices = self.buffer.sample(self.batch_size)\n",
    "\n",
    "        next_values = next_legal*self.q_network.predict(next_states)\n",
    "        next_action = tf.math.argmax(tf.where(next_values != 0,next_values,-np.inf), 1)\n",
    "        future_rewards = self.t_q_network.predict(next_states)\n",
    "        mask_next_action = tf.one_hot(next_action, self.n_actions)\n",
    "        # Q value = reward + discount factor * expected future reward\n",
    "        updated_q_values = rewards + self.gamma * tf.reduce_sum(tf.multiply(future_rewards, mask_next_action), axis=1)\n",
    "        \n",
    "        # set last q value to 0\n",
    "        updated_q_values = updated_q_values*(1 - dones)\n",
    "        masks = tf.one_hot(actions, self.n_actions)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "          # Train the model on the states and updated Q-values\n",
    "          q_values = self.q_network(states)\n",
    "          # only update q-value which is chosen\n",
    "          q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "          # calculate loss between new Q-value and old Q-value\n",
    "          loss = tf.reduce_mean(importance * tf.math.square(q_action - updated_q_values))\n",
    "        \n",
    "        # set priorities\n",
    "        errors = updated_q_values - q_action\n",
    "        self.buffer.set_priorities(indices, errors)\n",
    "        \n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))\n",
    "\n",
    "    def update_target_network(self):\n",
    "        # update per update_target_network steps\n",
    "        self.t_q_network.set_weights(self.q_network.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b0876-0c6a-4c70-bfa8-3df09095cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = (40,9,1)\n",
    "n_actions = 40\n",
    "update_per_actions = 4\n",
    "max_steps_per_episode = 1000\n",
    "update_target_network = 1000\n",
    "agent = DQN_agent(n_states, n_actions)\n",
    "env = Factory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "baed7924-4b84-4f0e-8acb-15abd2800748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:66, tardy percentage:0.115\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19028/1564790491.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# store training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         agent.store(\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_legal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19028/3663382522.py\u001b[0m in \u001b[0;36mstore\u001b[1;34m(self, state, action, next_state, reward, done, next_legal)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_legal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m# store training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_legal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19028/3687246356.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, experience)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperience\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpriorities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpriorities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#new experience has higher prob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_probabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(x, y, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1680\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1681\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_same_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1682\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1683\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mgreater\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   3941\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3942\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3943\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   3944\u001b[0m         _ctx, \"Greater\", name, x, y)\n\u001b[0;32m   3945\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episode = 0\n",
    "\n",
    "while True:\n",
    "    episode += 1\n",
    "    state, legal = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        agent.frame_count += 1\n",
    "        # choose action\n",
    "        action = agent.choose_action(state, legal)\n",
    "        # decay prob of exploration\n",
    "        agent.decay_epsilon()\n",
    "\n",
    "        next_state, reward, done, next_legal, inf = env.step(action)\n",
    "\n",
    "        episode_reward += reward\n",
    "        # store training data\n",
    "        agent.store(\n",
    "            np.expand_dims(state,axis=-1), action, np.expand_dims(next_state,axis=-1), \\\n",
    "                reward, done, next_legal\n",
    "            )\n",
    "\n",
    "        state = next_state\n",
    "        legal = next_legal\n",
    "        \n",
    "        if agent.frame_count % update_per_actions == 0 and len(agent.buffer.buffer) >= agent.batch_size:\n",
    "            agent.train_q_network()\n",
    "\n",
    "        if agent.frame_count % update_target_network == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "        if done:\n",
    "            print('episode:{}, tardy percentage:{}'.format(episode, inf))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7949931f-cf54-4eae-b26d-4994de355761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d359f1b-209b-4767-8962-86dda8b53dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
